ğŸ“Š Kaggle Playground Series - Season 5, Episode 1


ğŸ† Sticker Sales Forecasting


ğŸ”— Competition Link -- https://www.kaggle.com/competitions/playground-series-s5e1/data



ğŸš€ Overview

This project is my submission for the Kaggle Playground Series - Season 5, Episode 1 competition. The goal was to predict sticker sales in various countries and stores using historical sales data. The evaluation metric used was Mean Absolute Percentage Error (MAPE).

Final Rank: 2550
Score: 2.95087
Kaggle Profile: Ayushi_Raj




ğŸ“‚ Dataset
The dataset consists of sales data for Kaggle-branded stickers, including:

train.csv: Sales data with date, country, store, product, and num_sold.
test.csv: Data without num_sold, for which predictions were made.
sample_submission.csv: The required format for submission.





ğŸ›  Approach
ğŸ” Feature Engineering
Date Features: Extracted year, month, day, and day of the week.
Lag Features: Used lag_1 to capture previous day sales trends.
Rolling Statistics: Computed rolling_mean and rolling_median over 7-day windows.
Weekend Effect: Added an is_weekend feature.
One-Hot Encoding: Converted categorical features (country, store, product) into numerical form.


ğŸ¤– Models Used
1ï¸âƒ£ H2O AutoML â€“ Trained multiple models and selected the best-performing one.
2ï¸âƒ£ XGBoost with Hyperparameter Tuning â€“ Used RandomizedSearchCV for optimizing learning_rate, n_estimators, max_depth, etc.





ğŸ“ˆ Model Performance
The best model was XGBoost, achieving an MAE of 104.01 on the validation set.

Feature Importance:
rolling_mean and lag_1 were key predictors.
is_weekend improved the modelâ€™s ability to handle seasonality.




ğŸ“Œ Lessons Learned
ğŸ”¹ Feature engineering played a huge role in improving model performance.
ğŸ”¹ Hyperparameter tuning with RandomizedSearchCV helped optimize XGBoost.
ğŸ”¹ Using H2O AutoML provided insights into different model performances.
ğŸ”¹ Time-series forecasting requires careful handling of date-based trends.




ğŸ“ Files in This Repository
ğŸ“Œ notebook.ipynb â†’ Full Kaggle notebook with data preprocessing, model training, and predictions.
ğŸ“Œ train.csv & test.csv & sample_submission.csv â†’ Dataset 
ğŸ“Œ submission.csv â†’ Final predictions file submitted to Kaggle.
ğŸ“Œ README.md â†’ This file with project details.




ğŸ’¡ Future Improvements
âœ… Try LSTM/RNN-based models for sequential sales prediction.
âœ… Explore feature selection techniques to reduce overfitting.
âœ… Implement stacked ensemble models (e.g., blending XGBoost, LightGBM, and CatBoost).
âœ… Use time-based cross-validation instead of random splitting.






ğŸ”— Connect with Me
ğŸ’» GitHub: https://github.com/ayushiraj02
ğŸ“Š Kaggle: Ayushi_Raj
